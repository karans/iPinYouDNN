<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>The eGPU Experience by karans</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">The eGPU Experience</h1>
      <h2 class="project-tagline">Using an External GPU for Deep Learning</h2>
      <a href="https://github.com/karans/iPinYouDNN" class="btn">View on GitHub</a>
      <a href="https://github.com/karans/iPinYouDNN/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/karans/iPinYouDNN/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="gpus-in-deep-learning" class="anchor" href="#gpus-in-deep-learning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GPUs in Deep Learning</h3>

<p>There is a large market in businesses today with advancements in machine learning and artificial intelligence. With the developments in the algorithms and models that drive these concepts, a parallel use and development of the hardware to run and train these models is ongoing.</p>

<p>Researchers and those working on deep learning and big data related competitions often invest in graphics processing units to accelerate their workflow. These GPUs are built to handle parallel computation of tensors. When working on research utilizing these deep learning models, I decided to invest in one. </p>

<h3>
<a id="obtaining-a-setup" class="anchor" href="#obtaining-a-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Obtaining a Setup</h3>

<p>When choosing a GPU, there are many factors for which one works best for you, and are covered very well by <a href="http://timdettmers.com/2014/08/14/which-gpu-for-deep-learning/">Tim Dettmers</a>. I eventually chose a Nvidia Titan X, which had more than enough memory and bandwidth to run today's deep learning models. </p>

<p>The second task was deciding how to set it up. Since I commute often, I didn't want to invest in building a full computer setup. So the only other possibility was to find a way to hook up by GPU to the MacBook that I own externally. In this market there are not too many options, with the most common solution being a PCIe to Thunderbolt board. </p>

<p>There are a few companies that manufacture these solutions intended to host typical PCIe cards such as RAID cards or video capture cards, but don't supply enough power to run a GPU by default. Most people are able to take apart these platforms to add their own power supply and PCIe riser to overcome the power limit. Since I didn't want to spend too much time tinkering with the hardware, I opted for a prebuilt solution by <a href="https://bizon-tech.com/us/bizonbox2-egpu.html">Bizon</a>.</p>

<h3>
<a id="compatibility" class="anchor" href="#compatibility" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compatibility</h3>

<p>When I recieved my PCIe box, I was concerned that It would take a while to configure OSX with driver modifications. I also was questioning if it would work with Theano, a tensor python library I am writing my code with. </p>

<p>In terms of drivers, there setup was much easier than expected, as <a href="https://github.com/goalque/automate-eGPU">goalque</a> already wrote a script that detected the card, pulled the drivers from Nvidia and changed the appropriate IOPCI tunnelling files. When I rebooted the card was recognized!</p>

<p><img src="http://i.imgur.com/1ARjUeV.png%20=500x500" alt="System Info"></p>

<p>The reason why most people use Nvidia cards to run these sort of computations is due to CUDA. CUDA, proprietary to Nvidia, allows users to directly run code directly on the GPU. Many languages and packages targeted towards deep learning have integrated their libaries with CUDA to let users run demanding code on GPUs in the comfort of high level languages such as Python or Lua. Nvidia's cuDNN also optimizes many of the operations that occur during machine learning and have been utlized by libraries, such as Theano, as well. I then continued by installed CUDA (I recommend using the <a href="https://developer.nvidia.com/cuda-downloads">full</a> installation for those who just started) and cuDNN (you need to make a free developer account). </p>

<h3>
<a id="performance-restrictions" class="anchor" href="#performance-restrictions" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance Restrictions</h3>

<p>While having a portable setup is nice, I knew that there were going to be some restrictions. Since the card was running off of a thunderbolt connections, there are possilibities for bottlenecks, as Thunderbolt 2 maxes out at 2.5 GB/s. From the system configuration above, a card on PCIe 3.0 at 4x would usually be able to transfer at 4 GB/s.</p>

<p>Video cards get hot, especially with the stock cooler that comes with the Titan X. Usually decent radiators and cooling systems inside PCs can mitigate this problem. Since my box doesn't have any additional cooling, If I run the card for extended periods of time, which I hope to when training my models, I'm going to expect some extra throttling.</p>

<h3>
<a id="performance-tests" class="anchor" href="#performance-tests" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Performance Tests</h3>

<p>Regarless of the potential speed and thermal issues, I was quite pleased by how the card performed. Running convolutional_mlp, I was able to get a 45-50x speedup. </p>

<p><img src="http://i.imgur.com/2S7dG7E.png%20=500x300" alt="convmlp"></p>

<p>Running some synthetic benchmarks (I might test other ones in the future), I got about the same score as a Titan X would in a rig (could be in the margin of error due to different CPUs, PCIe configs, RAM, etc.).</p>

<p><img src="http://i.imgur.com/gYM4r9h.jpg%20=500x300" alt="OB"><img src="http://i.imgur.com/mGasect.png%20=500x300" alt="stats"></p>

<p>Finally when testing out my personal code, after adjusting some of the CUDA and Theano global variables, I got it to work smoothly. From from the different configuartions I was testing, MLP, CNN, RNN, I was able to get around a 23-30x speedup from the i7-4870HQ in my MacBook.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/karans/iPinYouDNN">The eGPU Experience</a> is maintained by <a href="https://github.com/karans">karans</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
